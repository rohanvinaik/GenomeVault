#!/usr/bin/env python3
"""
Generate pre-built projection memory maps for faster startup.
These cached projections significantly speed up encoder initialization.
"""

import argparse
import time
from pathlib import Path

import numpy as np

# Simple logger fallback for when genomevault package isn't installed
try:
    from genomevault.utils.logging import logger
except ImportError:
    from genomevault.observability.logging import configure_logging

    logger = configure_logging()
    logger.exception("Unhandled exception")
    import logging

    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
    )
    logger = logging.getLogger(__name__)
    raise


class ProjectionGenerator:
    """Generate and cache random projection matrices."""

    def __init__(self, cache_dir: Path):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def generate_projection(self, dimension: int, dtype=np.float32) -> np.ndarray:
        """Generate a random projection matrix."""
        logger.info(f"Generating {dimension}x{dimension} projection matrix...")

        # Use random normal distribution for projections
        # This is standard for random projection methods
        projection = np.random.randn(dimension, dimension).astype(dtype)

        # Normalize rows for stability
        row_norms = np.linalg.norm(projection, axis=1, keepdims=True)
        projection = projection / row_norms

        return projection

    def save_as_memmap(self, projection: np.ndarray, filename: str) -> Path:
        """Save projection matrix as memory-mapped file."""
        filepath = self.cache_dir / filename

        # Create memory-mapped file
        shape = projection.shape
        dtype = projection.dtype

        logger.info(f"Saving to {filepath} (shape={shape}, dtype={dtype})")

        # Write to disk
        memmap = np.memmap(filepath, dtype=dtype, mode="w+", shape=shape)
        memmap[:] = projection[:]
        memmap.flush()

        # Save metadata
        metadata_path = filepath.with_suffix(".meta.npy")
        np.save(
            metadata_path, {"shape": shape, "dtype": str(dtype), "dimension": shape[0]}
        )

        return filepath

    def generate_standard_projections(
        self, max_dimension: int = None
    ) -> list[tuple[int, Path]]:
        """Generate projections for standard dimensions used in GenomeVault.

        Args:
            max_dimension: Maximum dimension to generate (for memory-constrained environments)
        """
        standard_dimensions = [
            1000,  # Small test size
            10000,  # Base HDC dimension
            15000,  # Mid-level features
            20000,  # High-level semantic features
            50000,  # Large-scale analysis
            100000,  # Maximum dimension
        ]

        # Filter dimensions based on max_dimension if provided
        if max_dimension:
            standard_dimensions = [d for d in standard_dimensions if d <= max_dimension]
            logger.info(
                f"Limiting to dimensions <= {max_dimension} due to memory constraints"
            )

        generated_files = []

        for dim in standard_dimensions:
            logger.info(f"Generating projection for dimension {dim}")
            start_time = time.time()

            # Generate projection
            projection = self.generate_projection(dim)

            # Save as memmap
            filename = f"proj_{dim}.bin"
            filepath = self.save_as_memmap(projection, filename)

            elapsed = time.time() - start_time
            file_size_mb = filepath.stat().st_size / (1024 * 1024)

            logger.info(f"  Generated in {elapsed:.2f}s, size: {file_size_mb:.1f} MB")
            generated_files.append((dim, filepath))

        return generated_files

    def verify_projection(self, filepath: Path) -> bool:
        """Verify a projection file can be loaded correctly."""
        try:
            # Load metadata
            metadata_path = filepath.with_suffix(".meta.npy")
            metadata = np.load(metadata_path, allow_pickle=True).item()

            # Load projection as memmap
            shape = tuple(metadata["shape"])
            dtype = np.dtype(metadata["dtype"])

            projection = np.memmap(filepath, dtype=dtype, mode="r", shape=shape)

            # Basic checks
            assert projection.shape == shape
            assert projection.dtype == dtype

            # Check normalization (first few rows)
            for i in range(min(10, shape[0])):
                row_norm = np.linalg.norm(projection[i])
                assert abs(row_norm - 1.0) < 0.01, f"Row {i} not normalized: {row_norm}"

            logger.info(f"✓ Verified {filepath.name}")
            return True

        except Exception as e:
            from genomevault.observability.logging import configure_logging

            logger = configure_logging()
            logger.exception("Unhandled exception")
            logger.error(f"✗ Failed to verify {filepath.name}: {e}")
            return False
            raise

    def clean_old_projections(self, keep_latest: int = 1):
        """Remove old projection files, keeping only the latest versions."""
        # Group files by dimension
        dimension_files = {}

        for file_path in self.cache_dir.glob("proj_*.bin"):
            # Extract dimension from filename
            try:
                dim_str = file_path.stem.split("_")[1]
                dimension = int(dim_str)

                if dimension not in dimension_files:
                    dimension_files[dimension] = []
                dimension_files[dimension].append(file_path)
            except (IndexError, ValueError):
                from genomevault.observability.logging import configure_logging

                logger = configure_logging()
                logger.exception("Unhandled exception")
                continue
                raise

        # Clean old files
        for dimension, files in dimension_files.items():
            # Sort by modification time
            files.sort(key=lambda f: f.stat().st_mtime, reverse=True)

            # Remove old files
            for old_file in files[keep_latest:]:
                logger.info(f"Removing old projection: {old_file}")
                old_file.unlink()

                # Also remove metadata
                meta_file = old_file.with_suffix(".meta.npy")
                if meta_file.exists():
                    meta_file.unlink()


def main():
    parser = argparse.ArgumentParser(
        description="Generate cached projection matrices for GenomeVault"
    )
    parser.add_argument(
        "--max-dim",
        type=int,
        help="Maximum dimension to generate (for memory-constrained environments)",
    )
    parser.add_argument(
        "--dim",
        type=int,
        help="Specific dimension to generate (default: all standard dimensions)",
    )
    parser.add_argument(
        "--outfile", type=str, help="Output filename (only used with --dim)"
    )
    parser.add_argument(
        "--cache-dir",
        type=str,
        default=".cache",
        help="Cache directory (default: .cache)",
    )
    parser.add_argument(
        "--verify", action="store_true", help="Verify existing projections"
    )
    parser.add_argument(
        "--clean", action="store_true", help="Clean old projection files"
    )

    args = parser.parse_args()

    # Create generator
    generator = ProjectionGenerator(Path(args.cache_dir))

    if args.verify:
        # Verify existing projections
        logger.info("Verifying existing projections...")
        all_valid = True

        for proj_file in generator.cache_dir.glob("proj_*.bin"):
            if not generator.verify_projection(proj_file):
                all_valid = False

        if all_valid:
            logger.info("All projections verified successfully!")
        else:
            logger.error("Some projections failed verification")
            exit(1)

    elif args.clean:
        # Clean old files
        logger.info("Cleaning old projection files...")
        generator.clean_old_projections()

    elif args.dim:
        # Generate specific dimension
        if not args.outfile:
            args.outfile = f"proj_{args.dim}.bin"

        logger.info(f"Generating {args.dim}-dimensional projection...")
        projection = generator.generate_projection(args.dim)
        filepath = generator.save_as_memmap(projection, args.outfile)

        # Verify
        if generator.verify_projection(filepath):
            logger.info(f"Successfully generated: {filepath}")
        else:
            logger.error("Generated projection failed verification!")
            exit(1)

    else:
        # Generate all standard projections
        logger.info("Generating standard projections...")
        start_time = time.time()

        generated = generator.generate_standard_projections(max_dimension=args.max_dim)

        # Verify all
        logger.info("\nVerifying generated projections...")
        all_valid = True
        for dim, filepath in generated:
            if not generator.verify_projection(filepath):
                all_valid = False

        elapsed = time.time() - start_time
        logger.info(f"\nTotal generation time: {elapsed:.1f} seconds")

        if all_valid:
            logger.info("✅ All projections generated and verified successfully!")

            # Print summary
            print("\nGenerated projections:")
            print("-" * 50)
            for dim, filepath in generated:
                size_mb = filepath.stat().st_size / (1024 * 1024)
                print(f"  {dim:>6}D: {filepath.name} ({size_mb:.1f} MB)")
            print("-" * 50)
        else:
            logger.error("❌ Some projections failed verification")
            exit(1)


if __name__ == "__main__":
    main()

