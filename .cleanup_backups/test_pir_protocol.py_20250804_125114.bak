"""
Test suite for Information-Theoretic PIR Protocol.
Includes unit tests, adversarial tests, and performance benchmarks.
"""

import secrets
import time

import numpy as np
import pytest
from hypothesis import given
from hypothesis import strategies as st

from genomevault.pir.it_pir_protocol import (BatchPIRProtocol, PIRParameters,
                                             PIRProtocol)

# Constants for magic number elimination (PLR2004)
TIMING_VARIANCE_MAX = 5.0
MIN_SERVERS_TS = 2
MIN_SERVERS_LN = 3
TIMING_VARIANCE_LARGE = 10.0
QPS_MIN = 10


class TestPIRProtocol:
    """Unit tests for PIR protocol."""

    def setup_method(self):
        """Setup test parameters."""
        self.params = PIRParameters(
            database_size=1000, element_size=1024, num_servers=2
        )
        self.protocol = PIRProtocol(self.params)

        # Create test database
        self.database = np.random.randint(
            0,
            256,
            (self.params.database_size, self.params.element_size),
            dtype=np.uint8,
        )

    def test_parameter_validation(self):
        """Test parameter validation."""
        # Test invalid server count
        with pytest.raises(ValueError, match="at least 2 servers"):
            PIRParameters(database_size=100, num_servers=1)

        # Test invalid database size
        with pytest.raises(ValueError, match="must be positive"):
            PIRParameters(database_size=0, num_servers=2)

        # Test invalid element size
        with pytest.raises(ValueError, match="must be 1024"):
            params = PIRParameters(database_size=100, element_size=512)
            PIRProtocol(params)

    def test_query_vector_generation(self):
        """Test query vector generation correctness."""
        index = 42
        queries = self.protocol.generate_query_vectors(index)

        # Check number of queries
        assert len(queries) == self.params.num_servers

        # Check query dimensions
        for query in queries:
            assert len(query) == self.params.database_size
            assert query.dtype == np.uint8

        # Check that queries sum to unit vector
        sum_vec = np.zeros(self.params.database_size, dtype=np.uint8)
        for query in queries:
            sum_vec = (sum_vec + query) % 2

        # Should be unit vector at index
        expected = np.zeros(self.params.database_size, dtype=np.uint8)
        expected[index] = 1
        assert np.array_equal(sum_vec, expected)

    def test_server_response_processing(self):
        """Test server response computation."""
        index = 42
        queries = self.protocol.generate_query_vectors(index)

        # Process on each server
        responses = []
        for query in queries:
            response = self.protocol.process_server_response(query, self.database)
            assert len(response) == self.params.element_size
            responses.append(response)

        # Reconstruct should match original
        reconstructed = self.protocol.reconstruct_element(responses)
        assert np.array_equal(reconstructed, self.database[index])

    @given(st.integers(min_value=0, max_value=999))
    def test_retrieval_correctness(self, index: int):
        """Property test: retrieval is always correct."""
        queries = self.protocol.generate_query_vectors(index)

        responses = []
        for query in queries:
            response = self.protocol.process_server_response(query, self.database)
            responses.append(response)

        reconstructed = self.protocol.reconstruct_element(responses)
        assert np.array_equal(reconstructed, self.database[index])

    def test_query_padding(self):
        """Test query padding for fixed size."""
        index = 42
        query = self.protocol.generate_query_vectors(index)[0]

        padded = self.protocol.add_query_padding(query)

        # Check required fields
        assert "query_vector" in padded
        assert "padding" in padded
        assert "padded_size" in padded

        # Check padding creates fixed size
        assert padded["padded_size"] % self.params.padding_size == 0

    def test_timing_safe_response(self):
        """Test timing-safe response generation."""
        response = np.random.randint(0, 256, 1024, dtype=np.uint8)

        # Test multiple responses have similar timing
        timings = []
        for _ in range(10):
            padded, time_ms = self.protocol.timing_safe_response(
                response, target_time_ms=50
            )
            timings.append(time_ms)

            # Check fixed size
            assert len(padded) == self.params.element_size

        # Check timing variance is small
        timing_variance = np.var(timings)
        assert timing_variance < TIMING_VARIANCE_MAX  # Less than 5ms variance

    def test_privacy_calculations(self):
        """Test privacy breach probability calculations."""
        # Test with HIPAA TS nodes
        prob_ts = self.protocol.calculate_privacy_breach_probability(
            k_honest=2, honesty_prob=0.98
        )
        assert prob_ts == pytest.approx(0.0004, rel=1e-4)

        # Test with Light Nodes
        prob_ln = self.protocol.calculate_privacy_breach_probability(
            k_honest=3, honesty_prob=0.95
        )
        assert prob_ln == pytest.approx(0.000125, rel=1e-4)

        # Test minimum servers calculation
        min_ts = self.protocol.calculate_min_servers(1e-4, 0.98)
        assert min_ts == MIN_SERVERS_TS

        min_ln = self.protocol.calculate_min_servers(1e-4, 0.95)
        assert min_ln == MIN_SERVERS_LN


class TestAdversarialPIR:
    """Adversarial and side-channel tests."""

    def setup_method(self):
        """Setup test parameters."""
        self.params = PIRParameters(
            database_size=1000, element_size=1024, num_servers=2
        )
        self.protocol = PIRProtocol(self.params)
        self.database = np.random.randint(
            0,
            256,
            (self.params.database_size, self.params.element_size),
            dtype=np.uint8,
        )

    def test_malformed_query_length(self):
        """Test handling of malformed query vectors."""
        # Too short query
        short_query = np.random.randint(0, 2, 500, dtype=np.uint8)
        with pytest.raises(ValueError, match="size mismatch"):
            self.protocol.process_server_response(short_query, self.database)

        # Too long query
        long_query = np.random.randint(0, 2, 2000, dtype=np.uint8)
        with pytest.raises(ValueError, match="size mismatch"):
            self.protocol.process_server_response(long_query, self.database)

    def test_timing_attack_mitigation(self):
        """Test that timing doesn't leak information."""
        # Create queries for different indices
        indices = [10, 100, 500, 900]
        timings_per_index = []

        for idx in indices:
            queries = self.protocol.generate_query_vectors(idx)
            query = queries[0]

            # Measure multiple timings
            timings = []
            for _ in range(20):
                start = time.time()
                response = self.protocol.process_server_response(query, self.database)
                padded, _ = self.protocol.timing_safe_response(
                    response, target_time_ms=50
                )
                elapsed = (time.time() - start) * 1000
                timings.append(elapsed)

            timings_per_index.append(timings)

        # Check that timing distributions are similar across indices
        mean_timings = [np.mean(t) for t in timings_per_index]
        timing_variance = np.var(mean_timings)

        # Variance should be small (timing independent of index)
        assert timing_variance < TIMING_VARIANCE_LARGE  # Less than 10ms variance

    def test_collusion_simulation(self):
        """Test that colluding servers learn nothing."""
        index = 42
        queries = self.protocol.generate_query_vectors(index)

        # Simulate t-1 colluding servers (1 server for 2-server PIR)
        colluding_queries = queries[:1]

        # Try to determine index from colluding queries
        # This should be information-theoretically impossible

        # Count how many indices are consistent with observed queries
        consistent_indices = []
        for test_idx in range(self.params.database_size):
            # Generate queries for test index
            test_queries = self.protocol.generate_query_vectors(test_idx)

            # Check if first query matches
            if np.array_equal(test_queries[0], colluding_queries[0]):
                consistent_indices.append(test_idx)

        # Many indices should be consistent (no information gained)
        assert len(consistent_indices) > self.params.database_size / 10

    @given(st.lists(st.integers(0, 1), min_size=1000, max_size=1000))
    def test_fuzz_query_vectors(self, query_values: list[int]):
        """Fuzz test with random query vectors."""
        query = np.array(query_values, dtype=np.uint8)

        # Should not crash
        try:
            response = self.protocol.process_server_response(query, self.database)
            assert len(response) == self.params.element_size
        except ValueError:
            from genomevault.observability.logging import configure_logging

            logger = configure_logging()
            logger.exception("Unhandled exception")
            # Expected for invalid inputs
            pass
            raise

    def test_replay_attack_protection(self):
        """Test protection against replay attacks."""
        # Generate query with nonce
        index = 42
        query = self.protocol.generate_query_vectors(index)[0]
        padded_query = self.protocol.add_query_padding(query)

        # Add nonce
        nonce = secrets.token_hex(16)
        padded_query["nonce"] = nonce

        # First use should succeed
        response = self.protocol.process_server_response(query, self.database)
        assert response is not None

        # Replay detection would be handled by server handler
        # (checking nonce cache)


class TestBatchPIR:
    """Tests for batched PIR operations."""

    def setup_method(self):
        """Setup test parameters."""
        self.params = PIRParameters(
            database_size=10000, element_size=1024, num_servers=2
        )
        self.protocol = BatchPIRProtocol(self.params)
        self.database = np.random.randint(
            0,
            256,
            (self.params.database_size, self.params.element_size),
            dtype=np.uint8,
        )

    def test_batch_query_generation(self):
        """Test batch query generation."""
        indices = [10, 50, 100, 500, 1000]
        batch_queries = self.protocol.generate_batch_queries(indices)

        # Should generate queries for buckets
        assert len(batch_queries) > 0
        assert len(batch_queries) <= len(indices) * 3  # Cuckoo hashing bound

    def test_cuckoo_hashing(self):
        """Test cuckoo hashing for batch queries."""
        indices = list(range(100))
        num_buckets = 300

        buckets = self.protocol._cuckoo_hash(indices, num_buckets)

        # Check all indices are placed
        placed_indices = []
        for bucket in buckets:
            placed_indices.extend(bucket)

        # Most indices should be placed (some may fail in simplified implementation)
        assert len(placed_indices) >= len(indices) * 0.9


class TestPIRPerformance:
    """Performance benchmarks for PIR protocol."""

    def setup_method(self):
        """Setup test parameters."""
        self.params = PIRParameters(
            database_size=100000, element_size=1024, num_servers=2
        )
        self.protocol = PIRProtocol(self.params)
        self.database = np.random.randint(
            0,
            256,
            (self.params.database_size, self.params.element_size),
            dtype=np.uint8,
        )

    @pytest.mark.benchmark
    def test_query_generation_performance(self):
        """Benchmark query generation."""
        start_time = time.time()

        for _ in range(100):
            index = np.random.randint(0, self.params.database_size)
            queries = self.protocol.generate_query_vectors(index)

        elapsed = time.time() - start_time
        queries_per_second = 100 / elapsed

        print(f"\nQuery generation: {queries_per_second:.1f} queries/sec")
        assert queries_per_second > QPS_MIN  # Should generate >10 queries/sec

    @pytest.mark.benchmark
    def test_server_response_performance(self):
        """Benchmark server response computation."""
        index = 42
        query = self.protocol.generate_query_vectors(index)[0]

        start_time = time.time()

        for _ in range(10):
            response = self.protocol.process_server_response(query, self.database)

        elapsed = time.time() - start_time
        responses_per_second = 10 / elapsed

        print(f"Server response: {responses_per_second:.1f} responses/sec")
        print(f"Average latency: {elapsed / 10 * 1000:.1f}ms")

    @pytest.mark.benchmark
    def test_batch_performance(self):
        """Benchmark batch query performance."""
        batch_protocol = BatchPIRProtocol(self.params)
        indices = list(range(100))

        start_time = time.time()
        batch_queries = batch_protocol.generate_batch_queries(indices)
        elapsed = time.time() - start_time

        print(f"\nBatch query generation for 100 indices: {elapsed * 1000:.1f}ms")


if __name__ == "__main__":
    # Run tests
    pytest.main([__file__, "-v", "-s"])


